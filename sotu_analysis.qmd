---
title: "SOTU Analysis - POLI 176 Final Project"
format: html
editor: visual
---

```{r}
library(dplyr) 
```

# 1.

```{r}
sotu <- read.csv("SOTU_WithText.csv")

# filter from 1900s on 
filtered_sotu <- filter(sotu, year >= 1900)

set.seed(123123)
sample_n(filtered_sotu, 2)

```

```{r}
set.seed(12345)
sample_n(filtered_sotu, 2)
```

Select a corpus to study. Do some background reading on the corpus and decide on a research question that you can answer with the data that you have. What is your research question? What is the significance of your research question? Do a quick search -- has anyone studied this question before? (Please include at least 2 references) \[2-3 paragraphs\]

# 2.

Complete the steps below:

Step 1: Hand Coding

-   Define a categorization that would help you answer your research question. \[1-2 paragraphs\]

    -   public food assistance
    -   education
    -   public healthcare assistance

    Categories:

    -   favor
    -   not in favor
    -   neutral

-   Create a detailed codebook. \[Include codebook\]

    -   We need to figure out what metrics we are categorizing on:
    -   sentiment words

-   Select a random sample from your data of at least 50 documents for hand coding. Have at least two people code the data, read the data back into R, and assess inter-coder reliability. \[1-2 paragraphs plus metrics on intercoder reliability\]

```{r}
# Set your working directory
setwd("~/Documents/GitHub/SOTU-Text-Analysis")

# Read in handcoding
handcoding <- read_csv("handcoding.csv", show_col_types = FALSE)
handcoding <- handcoding[-c(1:6), ] # drop rows

# Rename columns for joining text from sotu df
handcoding <- handcoding %>%
  rename(year = Year, president = President, sotu_type = "SOTU Type")

# Standardize sotu year type (character)
sotu <- sotu %>%
  mutate(year = as.character(year), president = as.character(president))

# Join text to handcoding data
handcoding <- handcoding %>%
  right_join(sotu, by = c("year", "president", "sotu_type")) %>%
  select(-Text)
```

```{r}
## IRR
#Confusion matrix 
table(handcoding$Disapproval_1, handcoding$Disapproval_2)
table(handcoding$Neutral_1, handcoding$Neutral_2)
table(handcoding$Approval_1, handcoding$Approval_2)
```

```{r}
#Krippendorff's alpha
kripp.alpha(t(handcoding[,c("Disapproval_1", "Disapproval_2")]))
kripp.alpha(t(handcoding[,c("Neutral_1", "Neutral_2")]))
kripp.alpha(t(handcoding[,c("Approval_1", "Approval_2")]))
```

```{r}
# Create labels
handcoding$approval <- ifelse(handcoding$Approval_1==1 | handcoding$Approval_2==1, 1, 0)
table(handcoding$approval)

# Note: approval/disapproval is slightly imbalanced
# This a binary label (approval = 1, neutral or disapproval = 0)
```

-   Where did you codes conflict? \[1 paragraph\]

-   How would you revise your codebook based on these conflicts? \[1-2 paragraphs\]

Step 2: Measurement

(Choose A or B or both).

A: Use your hand coded data (you may need to hand code more training data) from step 1 to train a classifier to predict the categories you are interested in. Why did you choose the algorithm you did? \[1-2 paragraphs\] Use cross-validation to decide between variants of the models. Assess and report your out of sample precision and recall. \[1-2 paragraphs + table with precision and recall\]
```{r}
#Preprocess the data
corpus_sotu <- corpus(handcoding, text_field = "text")
corpus_sotu

#Create a document feature matrix (dfm)
#Some common pre-processing
toks <- tokens(corpus_sotu, remove_punct = TRUE, remove_numbers=TRUE)
toks <- tokens_wordstem(toks)
toks <- tokens_select(toks,  stopwords("en"), selection = "remove")
dfm <- dfm(toks)
```

```{r}
#Split into training, validation, and unlabeled data
#Create an ID variable
docvars(corpus_sotu, "id_numeric") <- 1:ndoc(corpus_sotu)

#Unlabeled data
unlabeled <- which(is.na(handcoding$approval))
#Labeled data
labeled <- which(!is.na(handcoding$approval))

#Sample training and validation set from the labeled data
set.seed(1234)
training <- sample(labeled, round(length(labeled)*.75))
length(training)
validation <- labeled[!labeled%in%training]
length(validation)

#Create separate dfm's for each
dfmat_train <- dfm_subset(dfm, docvars(corpus_sotu, "id_numeric")%in%training)
dfmat_val <- dfm_subset(dfm, docvars(corpus_sotu, "id_numeric")%in%validation)
```

```{r}
#Naive Bayes
#Train
tmod_nb <- textmodel_nb(dfmat_train, docvars(dfmat_train, "approval"))
summary(tmod_nb)
```
```{r}
#Words associated with international:
sort(coef_nb[,2]/coef_nb[,1], decreasing=T)[1:20]
```


```{r}
#Words not associated with international
sort(coef_nb[,2]/coef_nb[,1], decreasing=F)[1:20]
```
```{r}
#How well does it do in sample?
predict.train <- predict(tmod_nb, dfmat_train)

tab_train <- table(docvars(dfmat_train, "approval"), predict.train)
tab_train

#precision
diag(tab_train)/colSums(tab_train)
#recall
diag(tab_train)/rowSums(tab_train)

```
```{r}
#How well does this prediction do out of sample?  Validation
predict.val <- predict(tmod_nb, newdata = dfmat_val)

tab_val <- table(docvars(dfmat_val, "approval"), predict.val)
tab_val

#precision
diag(tab_val)/colSums(tab_val)
#recall
diag(tab_val)/rowSums(tab_val)

# We probably need more handcoding for better model robustness
```

```{r}
#Let's apply Naive Bayes to the whole dataset:
handcoding$predict.approval <- as.numeric(as.character(predict(tmod_nb, newdata = dfm)))

#Here is the prediction of approval for the whole corpus
prop.table(table(handcoding$predict.approval))

# We are underpredicting approval and overpredicting disapproval
prop.table(table(handcoding$approval))
```

B: Use a large language model to predict the categories instead. What prompt did you choose and why? \[1-2 paragraphs\] Use your hand coded data to assess and report your out of sample precision and recall. \[1-2 paragraphs + table with precision and recall\]

Apply either A or B to the whole corpus. Use the measure to answer your research question. \[1-2 paragraphs plus plot\] What do you learn? \[1-2 paragraphs\] What are some of the limitations of this approach? \[1-2 paragraphs\]

**References:**

Please include a works cited or reference list at the end of your memo, with in-text references to any other work that you reference throughout.All work should be properly cited and the written work should be your own -- not taken from another source or generated by AI.
