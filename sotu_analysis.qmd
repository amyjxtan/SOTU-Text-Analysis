---
title: "SOTU Analysis - POLI 176 Final Project"
format: html
editor: visual
---

```{r}
library(dplyr) 
library(tidyverse)
library(tokenizers)
library(quanteda)
library(quanteda.textmodels)
library(caret)
library(irr)
```

# 1.

```{r}
sotu <- read.csv("SOTU_WithText.csv")

# filter from 1900s on 
filtered_sotu <- filter(sotu, year >= 1900)

```

Select a corpus to study. Do some background reading on the corpus and decide on a research question that you can answer with the data that you have. What is your research question? What is the significance of your research question? Do a quick search -- has anyone studied this question before? (Please include at least 2 references) \[2-3 paragraphs\]

# 2.

-   For this project, we created 3 categories of sentiment in State of the Union speeches regarding public health care: approval, neutral, and disapproval. Our definitions are as follows:

    -   **Approval**: SOTU speech expressing satisfaction or belief in the value of a public health program or initiative. This can also take the form of having disapproval for private health initiatives (i.e. lack of support of the privatization of health care programs).
    -   **Neutral**: SOTU speech that does not explicitly express support or disapproval of a public health program or initiative. This also includes speeches that do not explicitly/specifically reference public or private healthcare, or healthcare at all.
    -   **Disapproval**: SOTU speech expressing dissatisfaction or highlighting shortcomings, failures, or barriers related to a public health initiative. This can also take the form of having approval for private health initiatives (i.e. support for the privatization of health care programs).

    Additionally, we created a code book with these definitions to further clarify these categories. The code book also includes some keywords and example quotes from SOTU speeches for reference.

| **Sentiment** | **Definition** | **Keywords** | **Example** |
|------------------|------------------|------------------|--------------------|
| Approval | SOTU speech expressing satisfaction or belief in the value of a public health program or initiative. This can also take the form of having disapproval for private health initiatives (i.e. lack of support of the privatization of health care programs) | "support", "advocate", "commitment", "achieve", "improve", "success", "priority", "benefit", "impact", "significant progress", "solution", "progress", "invest", "improved health", "efforts have paid off" -- note: negation in conjunction with these words would be associated with disapproval | "Our Government has a responsibility to provide health care for the poor and the elderly, and we are meeting that responsibility. For all Americans, we must confront the rising cost of care, strengthen the doctor-patient relationship, and help people afford the insurance coverage they need." |
| Neutral | SOTU speech that does not explicitly express support or disapproval of a public health program or initiative. This also includes speeches that do not explicitly/specifically reference public or private healthcare, or healthcare at all. | "address", "discuss", "work", "plan", "explore", "next steps", "looking forward", "consider" | "I see a new America as we celebrate our 200th anniversary 6 years from now. I see an America in which we have abolished hunger, provided the means for every family in the Nation to obtain a minimum income, made enormous progress in providing better housing, faster transportation, improved health, and superior education." |
| Disapproval | SOTU speech expressing dissatisfaction or highlighting shortcomings, failures, or barriers related to a public health initiative. This can also take the form of having approval for private health initiatives (i.e. support for the privatization of health care programs) | "fail", "struggle", "insufficient", "lack", "decline", "not working", "unmet", "challenges", "shortcomings", "neglect", "ineffective", "difficult", "hurdle", "cut", "undermining", "deficient", etc. -- note: negation in conjunction with these words would be associated with approval | "In all we do, we must remember that the best health care decisions are not made by government and insurance companies but by patients and their doctors." |

After creating this code book, we selected a random sample of 50 documents from the SOTU corpus for hand coding. Out of the 50 speeches, 29 of these speeches were from Republicans, and the speeches ranged from 1905 to 2013. Using this sample, two members of the group independently coded the 50 speeches according to the handbook and their interpretation of the text. The hand coded data was read back into R, and we computed confusion matrices and Krippendorff's alpha for all three categories to assess intercoder reliability.

-   

    ```{r}
    # random sample of 50
    set.seed(54321)
    random_50 <- sample_n(filtered_sotu, 50)

    # get num of Republican speeches in the sample
        rep <- filter(random_50, party == 'Republican')

    # read handcoded results back into R
    handcode <- read.csv('handcoded.csv')
    random_50$Disapproval_1 <- handcode$Disapproval_1
    random_50$Neutral_1 <- handcode$Neutral_1
    random_50$Approval_1 <- handcode$Approval_1
    random_50$Disapproval_2 <- handcode$Disapproval_2
    random_50$Neutral_2 <- handcode$Neutral_2
    random_50$Approval_2 <- handcode$Approval_2

    # Disapproval Confusion Matrix
    table(random_50$Disapproval_1, random_50$Disapproval_2)
    # Krippendorff's alpha - 0.912
    kripp.alpha(t(random_50[,c("Disapproval_1", "Disapproval_2")]))

    # Approval Confusion Matrix
    table(random_50$Approval_1, random_50$Approval_2)
    # Krippendorff's alpha - 0.735
    kripp.alpha(t(random_50[,c("Approval_1", "Approval_2")]))

    # Neutral Confusion Matrix
    table(random_50$Neutral_1, random_50$Neutral_2)
    # Krippendorff's alpha - 0.801 
    kripp.alpha(t(random_50[,c("Neutral_1", "Neutral_2")]))

    ```


We had the highest intercoder reliability in regards to the Disapproval category. Out of the 50 documents, our coders only conflicted on 1, with a Krippendorff's alpha or 0.912. Our coders agreed that 6 were Disapproval and 43 were not Disapproval. The second highest intercoder reliability was with the Neutral category. Out of the 50 documents, our coders conflicted on 5 documents. They agreed that 24 were Neutral, and 21 were not Neutral. The Krippendorff's alpha was 0.801.

-   Where did you codes conflict? \[1 paragraph\]

Across all three categories, our codes conflict the most in the Approval category, where we disagree on the coding of 6 out of 50 documents. This means that we disagreed the most on identifying speeches that were in approval of public healthcare. The category with the second most handcoding conflicts is the Neutral category, in which we disagreed on 5 of 50 documents. Finally, the category with the least handcoding conflicts is the Disapproval category, where we only disagreed on 1 of 50 documents.

-   How would you revise your codebook based on these conflicts? \[1-2 paragraphs\]

Step 2: Measurement

(Choose A or B or both).

A: Use your hand coded data (you may need to hand code more training data) from step 1 to train a classifier to predict the categories you are interested in. Why did you choose the algorithm you did? \[1-2 paragraphs\] Use cross-validation to decide between variants of the models. Assess and report your out of sample precision and recall. \[1-2 paragraphs + table with precision and recall\]

```{r}
### Preprocessing dataset
# Read in handcoding
handcoding <- read_csv("handcoding.csv", show_col_types = FALSE)
handcoding <- handcoding[-c(1:6), ] # drop rows

# Rename columns for joining text from sotu df
handcoding <- handcoding %>%
rename(year = Year, president = President, sotu_type = "SOTU Type")

# Standardize sotu year type (character)
sotu <- sotu %>%
mutate(year = as.character(year), president = as.character(president))

# Join text to handcoding data
handcoding <- handcoding %>%
right_join(sotu, by = c("year", "president", "sotu_type")) %>%
select(-Text)

# Create labels
handcoding$approval <- ifelse(handcoding$Approval_1==1 | handcoding$Approval_2==1, 1, 0)
table(handcoding$approval)

head(handcoding)
```

```{r}
table(handcoding$approval)
# Note: approval/disapproval is slightly imbalanced
# This a binary label (approval = 1, neutral or disapproval = 0)
```

```{r}
#Preprocess the data
corpus_sotu <- corpus(handcoding, text_field = "text")
corpus_sotu

#Create a document feature matrix (dfm)
#Some common pre-processing
toks <- tokens(corpus_sotu, remove_punct = TRUE, remove_numbers=TRUE)
toks <- tokens_wordstem(toks)
toks <- tokens_select(toks,  stopwords("en"), selection = "remove")
dfm <- dfm(toks)

#Split into training, validation, and unlabeled data
#Create an ID variable
docvars(corpus_sotu, "id_numeric") <- 1:ndoc(corpus_sotu)
#Unlabeled data
unlabeled <- which(is.na(handcoding$approval))
#Labeled data
labeled <- which(!is.na(handcoding$approval))
```


```{r}
#Sample training and validation set from the labeled data
set.seed(1234)
training <- sample(labeled, round(length(labeled)*.75))
length(training)
validation <- labeled[!labeled%in%training]
length(validation)

#Create separate dfm's for each
dfmat_train <- dfm_subset(dfm, docvars(corpus_sotu, "id_numeric")%in%training)
dfmat_val <- dfm_subset(dfm, docvars(corpus_sotu, "id_numeric")%in%validation)
```


```{r}
#Naive Bayes
#Train classifier
tmod_nb <- textmodel_nb(dfmat_train, docvars(dfmat_train, "approval"))
summary(tmod_nb)
```
```{r}
#Words associated with approval
sort(coef_nb[,2]/coef_nb[,1], decreasing=T)[1:20]
```


```{r}
#Words not associated with non-approval
sort(coef_nb[,2]/coef_nb[,1], decreasing=F)[1:20]
```
```{r}
#How well does it do in sample? - on training data
predict.train <- predict(tmod_nb, dfmat_train)
tab_train <- table(docvars(dfmat_train, "approval"), predict.train)
tab_train
#precision
diag(tab_train)/colSums(tab_train)
#recall
diag(tab_train)/rowSums(tab_train)
```
```{r}
#How well does this prediction do out of sample?  Validation
predict.val <- predict(tmod_nb, newdata = dfmat_val)
tab_val <- table(docvars(dfmat_val, "approval"), predict.val)
tab_val
#precision
diag(tab_val)/colSums(tab_val)
#recall
diag(tab_val)/rowSums(tab_val)

# We probably need more handcoding for better model robustness
```
```{r}
#Apply Naive Bayes to the whole dataset:
handcoding$predict.approval <- as.numeric(as.character(predict(tmod_nb, newdata = dfm)))

#Here is the prediction of approval for the whole corpus
prop.table(table(handcoding$predict.approval))

# If handcoding is representative of whole corpus,
# We are underpredicting approval and overpredicting disapproval
prop.table(table(handcoding$approval))
```

B: Use a large language model to predict the categories instead. What prompt did you choose and why? \[1-2 paragraphs\] Use your hand coded data to assess and report your out of sample precision and recall. \[1-2 paragraphs + table with precision and recall\]

Apply either A or B to the whole corpus. Use the measure to answer your research question. \[1-2 paragraphs plus plot\] What do you learn? \[1-2 paragraphs\] What are some of the limitations of this approach? \[1-2 paragraphs\]

**References:**

Please include a works cited or reference list at the end of your memo, with in-text references to any other work that you reference throughout.All work should be properly cited and the written work should be your own -- not taken from another source or generated by AI.
